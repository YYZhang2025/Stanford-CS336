{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9b8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"low low low low low\n",
    "lower lower widest widest widest <|endoftext|>\n",
    "newest newest newest newest newest newest\n",
    "\"\"\"\n",
    "\n",
    "medium_text = \"\"\"\n",
    "He said, “Wow, that is a really amazing vase! Can I buy it?”\n",
    "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
    "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was.\n",
    "And that's how Ben found an amazing vase in the store!\n",
    "<|endoftext|>\n",
    "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
    "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
    "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n",
    "<|endoftext|>\n",
    "One day, a little boy named Tim went to the park. He saw a big tiger. The tiger was not mean, but very easy to play with. Tim and the tiger played all day. They had lots of fun.\n",
    "Then, something unexpected happened. The tiger started to shake. Tim was scared. He did not know what was going on. But then, the tiger turned into a nice dog. Tim was very surprised.\n",
    "Tim and the dog played together now. They were very happy. The dog was easy to play with too. At the end of the day, Tim went home with his new friend.\n",
    "<|endoftext|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54684ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def split_by_special_tokens(text: str, special_tokens: list[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text by special tokens, preserving the tokens themselves.\n",
    "    \"\"\"\n",
    "    special_tokens_sorted = sorted(special_tokens, key=len, reverse=True)\n",
    "\n",
    "    if not special_tokens_sorted:\n",
    "        parts = [text]\n",
    "    else:\n",
    "        pattern = \"|\".join(re.escape(tok) for tok in special_tokens_sorted)\n",
    "        print(f\"Using pattern: {pattern}\")\n",
    "        parts = re.split(\"(\" + pattern + \")\", text)\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c0e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pattern: <\\|endoftext\\|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['low low low low low\\nlower lower widest widest widest ',\n",
       " '<|endoftext|>',\n",
       " '\\nnewest newest newest newest newest newest\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_special_tokens(text, [\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "\n",
    "# 1. Count the number of words in the text\n",
    "word_counts = Counter(text.split())\n",
    "\n",
    "\n",
    "def to_tuple(word: str) -> Tuple[bytes, ...]:\n",
    "    return tuple(bytes([b]) for b in word.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def flatten_bytes(tup: Tuple[bytes, ...]) -> str:\n",
    "    return \"\".join(b.decode(\"utf-8\") for b in tup)\n",
    "\n",
    "\n",
    "def bpe_merge(corpus: dict[Tuple[bytes, ...], int], num_merges: int):\n",
    "    merges = []\n",
    "    vocab = {}\n",
    "\n",
    "    # 1. 初始化 vocab：每个单字节 bytes → ID (0–255)\n",
    "    idx = 0\n",
    "    for word in corpus:\n",
    "        for token in word:\n",
    "            if token not in vocab.values():\n",
    "                vocab[idx] = token\n",
    "                idx += 1\n",
    "    next_token_id = max(vocab.keys(), default=-1) + 1\n",
    "\n",
    "    for step in range(num_merges):\n",
    "        pair_freq = Counter()\n",
    "\n",
    "        # Count adjacent pairs\n",
    "        for word, freq in corpus.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "\n",
    "        if not pair_freq:\n",
    "            break\n",
    "\n",
    "        # Most frequent pair (tie-break lexicographically)\n",
    "        most_common = pair_freq.most_common()\n",
    "        max_freq = most_common[0][1]\n",
    "        top_pairs = [p for p, c in most_common if c == max_freq]\n",
    "        pair_to_merge = max(top_pairs)\n",
    "\n",
    "        merges.append(pair_to_merge)\n",
    "\n",
    "        # Add merged token to vocab\n",
    "        merged_token = pair_to_merge[0] + pair_to_merge[1]\n",
    "        vocab[next_token_id] = merged_token\n",
    "        next_token_id += 1\n",
    "\n",
    "        # Merge all occurrences\n",
    "        new_corpus = {}\n",
    "        for word, freq in corpus.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:\n",
    "                    new_word.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus[tuple(new_word)] = freq\n",
    "\n",
    "        corpus = new_corpus\n",
    "\n",
    "        print(f\"Step {step + 1}: merge '{flatten_bytes(pair_to_merge)}'\")\n",
    "        print(\"Corpus:\")\n",
    "        for w, c in corpus.items():\n",
    "            print(f\"  {flatten_bytes(w)}: {c}\")\n",
    "        print()\n",
    "\n",
    "    return corpus, merges, vocab\n",
    "\n",
    "\n",
    "corpus = {to_tuple(word): count for word, count in word_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1b5364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'l',\n",
       " 1: b'o',\n",
       " 2: b'w',\n",
       " 3: b'e',\n",
       " 4: b'r',\n",
       " 5: b'i',\n",
       " 6: b'd',\n",
       " 7: b's',\n",
       " 8: b't',\n",
       " 9: b'n',\n",
       " 10: b'st',\n",
       " 11: b'est',\n",
       " 12: b'ow',\n",
       " 13: b'low',\n",
       " 14: b'west',\n",
       " 15: b'ne'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1641b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: merge 'st'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n",
      "Step 2: merge 'est'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n",
      "Step 3: merge 'ow'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n",
      "Step 4: merge 'low'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n",
      "Step 5: merge 'west'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n",
      "Step 6: merge 'ne'\n",
      "Corpus:\n",
      "  low: 5\n",
      "  lower: 2\n",
      "  widest: 3\n",
      "  newest: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_corpus, merge_history, final_vocab = bpe_merge(corpus, num_merges=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c749afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab:\n",
      "0: b'l' → 'l'\n",
      "1: b'o' → 'o'\n",
      "2: b'w' → 'w'\n",
      "3: b'e' → 'e'\n",
      "4: b'r' → 'r'\n",
      "5: b'i' → 'i'\n",
      "6: b'd' → 'd'\n",
      "7: b's' → 's'\n",
      "8: b't' → 't'\n",
      "9: b'n' → 'n'\n",
      "10: b'st' → 'st'\n",
      "11: b'est' → 'est'\n",
      "12: b'ow' → 'ow'\n",
      "13: b'low' → 'low'\n",
      "14: b'west' → 'west'\n",
      "15: b'ne' → 'ne'\n"
     ]
    }
   ],
   "source": [
    "print(\"Final vocab:\")\n",
    "for k, v in final_vocab.items():\n",
    "    print(f\"{k}: {v} → '{v.decode('utf-8', errors='replace')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6fcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
